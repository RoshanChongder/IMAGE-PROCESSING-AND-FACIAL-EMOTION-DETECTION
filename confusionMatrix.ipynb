{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "confusionMatrix.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1TyDiXEtBQbXiTcJW9nimin3scA0H9Fjs",
      "authorship_tag": "ABX9TyMRWP5lHoXhegnZTJW7mbTG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoshanChongder/IMAGE-PROCESSING-AND-FACIAL-EMOTION-DETECTION/blob/master/confusionMatrix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7saPqm08VH96"
      },
      "source": [
        "First , we will load the trained model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJjM3O46VTGv"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md3chI-LNfdU"
      },
      "source": [
        "# import the required libraries \n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2 , os , time , html \n",
        "import numpy as np\n",
        "import PIL , io\n",
        "from keras.preprocessing import image\n",
        "from keras.models import model_from_json "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRB1N-yqNnEu",
        "outputId": "403eeec1-3c5d-43ca-e510-3d20284b26c0"
      },
      "source": [
        "#drive/MyDrive/EmotionDetection/Model_Training_Op/24_06_1/model/\n",
        "model = model_from_json( open('drive/MyDrive/EmotionDetection/Model_Training_Op/24_06_1/model/fer.json','r').read() )\n",
        "# drive/MyDrive/EmotionDetection/Model_Training_Op/24_06_1/model/\n",
        "model.load_weights( 'drive/MyDrive/EmotionDetection/Model_Training_Op/24_06_1/model/fer.h5' )\n",
        "print('The model is been loadded with weights')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model is been loadded with weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOfZSdiLz5I_"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8OdD7LvN92H"
      },
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "  return img\n",
        "\n",
        "\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  bbox_PIL.save(iobuf, format='png') # format bbox into png for return\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))  # format return string\n",
        "\n",
        "  return bbox_bytes\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPphwyLAN-oJ"
      },
      "source": [
        "# Haar Cascade face detection model\n",
        "face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs8Wm3DAOCHx"
      },
      "source": [
        "\n",
        "def take_photo( loc , quality=0.8 ):\n",
        "\n",
        "  ''' This method receives a location of an image .\n",
        "  Then it reads the image and then predicts the emotion of \n",
        "  the face present in the image ''' \n",
        "\n",
        "\n",
        "  # read the image from the drive location that is passed \n",
        "  img = cv2.imread( loc , cv2.IMREAD_COLOR )\n",
        "  \n",
        "  # convertion to grayscale \n",
        "  gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "  #print(gray_img.shape)\n",
        "\n",
        "  # get face bounding box coordinates using Haar Cascade\n",
        "  faces = face_cascade.detectMultiScale(gray_img)\n",
        "  \n",
        "  # create transparent overlay for bounding box\n",
        "  bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "  \n",
        "  predicted = []\n",
        "\n",
        "  # draw face bounding box on image\n",
        "  for (x,y,w,h) in faces:\n",
        "\n",
        "      img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) # placing the rectangle around the face \n",
        "      cropped_gray = gray_img[y:y+w,x:x+h]  # cropping the face from the image \n",
        "      cropped_gray = cv2.resize(cropped_gray,(48,48)) # resizing the image \n",
        "      img_pixels = image.img_to_array(cropped_gray)  # getting the image pixeles into array \n",
        "      img_pixels = np.expand_dims(img_pixels, axis = 0) # expanding dim \n",
        "      img_pixels /= 255\n",
        "\n",
        "      predictions = model.predict(img_pixels)  # predicting \n",
        "        #print( predictions )\n",
        "        #find max indexed array\n",
        "      index = np.argmax(predictions[0]) \n",
        "        \n",
        "      emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
        "      predicted_emotion = emotions[index]  \n",
        "      predicted.append( index  )\n",
        "      #print( predicted_emotion )\n",
        "      cv2.putText( img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
        " \n",
        "  return [ img , predicted ] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywNuhNuqOPZo"
      },
      "source": [
        "# from where all the images will be read \n",
        "directory = 'drive/MyDrive/EmotionDetection/Model_Training_Op/images'   \n",
        "\n",
        "# d is for directory refference and tag  \n",
        "# actual and pre will store actual emotion and predicted emotion \n",
        "d , actual , pre = { \"angry\" : 0 , \"happy\" : 3 , \"sad\" : 4 , \"surprise\" : 5 , \"neutral\" : 6 } , [] , [] \n",
        "\n",
        "for dir in os.listdir(directory) : \n",
        "  tag = None \n",
        "  \n",
        "  if dir in d :\n",
        "    tag = d[dir]  # getting the tag correspondin to the dir  \n",
        "  else :\n",
        "    print( dir , \" not found in \" , d )\n",
        "    break \n",
        "\n",
        "  subdir = os.path.join( directory, dir ) # selecting a certain sub-directory \n",
        "  print( \"Processing the direcotry : \" , subdir , \"\\n\\n\")\n",
        "\n",
        "  for files in os.listdir(subdir) :\n",
        "    try :\n",
        "      name = os.path.join( subdir , files ) # name of the file \n",
        "      if os.path.isfile( name ):\n",
        "        img , predicted = take_photo( name )  # calling take with image location in drive  \n",
        "        if len(predicted) != 0 :\n",
        "          print( \"Actual \" , tag , \"Predicted value \" , predicted )  # printing the predicted value\n",
        "          actual.append(tag) # for debugging \n",
        "          pre.append(  tag if ( tag in predicted ) else predicted[0]  )\n",
        "          print(actual[-1] , pre[-1]) # for debugging \n",
        "          cv2.imwrite( subdir + \"/predicted/\" + files , img  ) \n",
        "        else :\n",
        "          print(\"Model not able to predict for \" , name )\n",
        "    except Exception as err:\n",
        "      # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "      # grant the page permission to access it.\n",
        "      print(str(err))\n",
        "\n",
        "print(actual , pre )\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyFYHuxfb-O3"
      },
      "source": [
        "print(actual)\n",
        "print(pre)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34RqZyl9cWqz"
      },
      "source": [
        "%matplotlib inline \n",
        "import matplotlib.pyplot as plt  \n",
        "import itertools \n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkD3r0mOdZZJ"
      },
      "source": [
        "c_matrix = confusion_matrix(y_true = actual , y_pred = pre )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCJnZqt-dnsN"
      },
      "source": [
        "def plot_confusion_matrix( cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.colorbar()\n",
        "    plt.title(title)\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UrxkJ0RebTm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34985ac5-00dd-4ce3-84d5-91e166169c31"
      },
      "source": [
        "plot_labels = ['angry', 'happy', 'sad', 'surprise', 'neutral']\n",
        "print(type(c_matrix))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPUoJFXhekoF"
      },
      "source": [
        "plot_confusion_matrix( cm = c_matrix , classes = plot_labels , title='Confusion Matrix' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXgJvFdhcGPI"
      },
      "source": [
        "import pandas as pd \n",
        "\n",
        "# Calculating of precision , recall and f1 score for each emotion\n",
        "TP = lambda cm , i : cm[i][i] # returning only for the ith emotion\n",
        "FP = lambda cm , i : sum([ cm[j][i] if j!=i else 0 for j in range(len(cm)) ])\n",
        "FN = lambda cm , i : sum([ cm[i][j] if j!=i else 0 for j in range(len(cm)) ]) \n",
        "f1 = lambda p,r : 2*(p*r)/(p+r) \n",
        "TotalSample = lambda cm : sum( [ cm[i][j] for i in range(len(cm))  for j in range(len(cm)) ] )\n",
        "\n",
        "data = [ ]\n",
        "for i in range(len(plot_labels)) :\n",
        "  temp =[] \n",
        "  # traversing through each emotion on index \n",
        "  temp.append(plot_labels[i])\n",
        "\n",
        "  precision = TP(c_matrix , i ) / ( TP(c_matrix , i ) + FP(c_matrix , i ) )\n",
        "  temp.append(round(precision,2))\n",
        "\n",
        "  recall = TP(c_matrix , i ) / ( TP(c_matrix , i ) + FN(c_matrix , i ) ) \n",
        "  temp.append(round(recall,2))\n",
        "   \n",
        "  temp.append(round( f1(precision,recall) , 2 ))\n",
        "  data.append(temp)\n",
        "\n",
        "# accuracy \n",
        "accuracy = sum( [ TP(c_matrix,i) for i in range(len(plot_labels)) ] ) / TotalSample(c_matrix) \n",
        "print('Accuracy : ' , round(accuracy,2) )\n",
        "\n",
        "df = pd.DataFrame( data , columns = [ 'Emotions' , 'Precision' , 'Recall' , 'F1 Score'] )\n",
        "df\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib4FjPPzs--8"
      },
      "source": [
        "from sklearn.metrics import classification_report \n",
        "print( classification_report( y_true = actual , y_pred = pre , target_names = plot_labels ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHCvZnwErzzn"
      },
      "source": [
        "Plotting confusion matrix in percentage form "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daK0umh_r4e_"
      },
      "source": [
        "# this can also be done using the sklearn library \n",
        "def percent_confusion_matrix( cm ) :\n",
        "  pcm = []   # confusion matrix in percentage \n",
        "  for i in cm :\n",
        "    total_images , temp = sum(i) , [] \n",
        "    for j in i :\n",
        "      temp.append(round(j/total_images , 2 ) )\n",
        "    pcm.append(temp)\n",
        "  return pcm\n",
        "plot_confusion_matrix( cm = np.array(percent_confusion_matrix(c_matrix)) , classes = plot_labels , title=' Confusion Matrix in percentage ' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh6hDs2RyRva"
      },
      "source": [
        "Computing the same using sklearn "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GC15RR9vwbKw"
      },
      "source": [
        "nc_matrix = confusion_matrix( y_true = actual , y_pred = pre , normalize = 'true' )\n",
        "print(nc_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}