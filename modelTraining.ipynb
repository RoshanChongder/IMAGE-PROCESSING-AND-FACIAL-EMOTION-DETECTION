{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modelTraining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "160LeSnO0EpcaQEZFwFuHXU7iePbPwb5B",
      "authorship_tag": "ABX9TyMNqv1aMvCpLAPbrYhy27+B",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoshanChongder/IMAGE-PROCESSING-AND-FACIAL-EMOTION-DETECTION/blob/master/modelTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHbjiJ3emIQK"
      },
      "source": [
        "To Mount Google Drive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veJuAiGEVQCO"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urBA-HZCfGhQ"
      },
      "source": [
        "Importing required Modules "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyFDerYCfJ7J"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# keras is free source deep learning library in python\n",
        "# from this library different layer will be imported\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPool2D, BatchNormalization, AveragePooling2D\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import np_utils\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stRhs7HuEbha"
      },
      "source": [
        "Downloading the csv file from the mounted google drive "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "354Xe42eYugK"
      },
      "source": [
        "Importing the data set and pre-viewing  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzifmEiiY1X7"
      },
      "source": [
        "# Method to read CSV Files\n",
        "def read_CSV( path ):\n",
        "    try:\n",
        "        file = pd.read_csv( path )\n",
        "        return file\n",
        "    except FileNotFoundError:\n",
        "        print( \"CSV File not found at \" + path )\n",
        "        return None\n",
        "    except Exception:\n",
        "        print(\" Unknown error appeared \")\n",
        "        return None\n",
        "\n",
        "# reading the csv file \n",
        "\n",
        "data_set = read_CSV('drive/MyDrive/EmotionDetection/fer2013/fer2013.csv') \n",
        "print( data_set.info() ) # checking info of the data set \n",
        "print( data_set.head())  # showing the first ew line of the data set \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gluCxUCBtrID"
      },
      "source": [
        "Now , as the data set is loaded , we will just extract the data from the csv file and tehn load then in  train , ytrain , xtest and ytest . \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90yZ2Limt2TZ"
      },
      "source": [
        "# Addition of data into list from CSV for training and public testing\n",
        "def data_Addition() :\n",
        "    global x_train , y_train , x_test , y_test , data_set\n",
        "    d = {3:0} \n",
        "    for row_count,row in data_set.iterrows():\n",
        "        #if row['emotion'] in [1,2] or ( d[3]>5800 and row['emotion']==3 and row['Usage'] == 'Training' ) :  continue \n",
        "        \n",
        "        if row['emotion'] in [1,2] or ( row['emotion'] == 3  and d[ row['emotion'] ] > 4000  and row['Usage'] == 'Training' ) : continue \n",
        "        value = row['pixels'].split(' ')   # extracting the pixels as a list\n",
        "        try :\n",
        "            if 'Training' in row['Usage'] :        # if the current column is for Training\n",
        "                x_train.append(np.array(value,'float32'))        # adding the pixels in the x axis\n",
        "                y_train.append(row['emotion'])\n",
        "\n",
        "                if row['emotion'] in d :\n",
        "                    d[ row['emotion'] ]+=1\n",
        "                else :\n",
        "                    d[ row['emotion'] ] = 1 \n",
        "\n",
        "                #{3: 5801, 0: 3995, 4: 4830, 6: 4965, 5: 3171}\n",
        "                # added this to restrict some of the happy data - ( d[3]>5800 and row['emotion']==3 and row['Usage'] == 'Training' ) \n",
        "\n",
        "            # adding emotion in the y axis\n",
        "            # elif 'PublicTest' in row['Usage']:    # if the current column is for testing\n",
        "            #     x_test.append(np.array( value,'float32'))\n",
        "            #     y_test.append( row['emotion'])\n",
        "\n",
        "                # {0: 467, 4: 653, 6: 607, 3: 895, 5: 415}\n",
        "            else : \n",
        "                # also including the private test set  over here \n",
        "                x_test.append(np.array( value,'float32'))\n",
        "                y_test.append( row['emotion']) \n",
        "\n",
        "                # {0: 958, 4: 1247, 6: 1233, 3: 1774, 5: 831}  # Testing :  6043 \n",
        "\n",
        "        except:\n",
        "            print(\" Error occurred at row number \" + row_count)\n",
        "            print(\"Data Set in that row is \" + row )\n",
        "\n",
        "    print(d)\n",
        "\n",
        "# Now we will do training and public testing\n",
        "\n",
        "x_train , y_train = [] , []   # data the will be used for training will added in this two lists\n",
        "x_test , y_test = [] , []     # data that will be used for public testing will be added here\n",
        "\n",
        "data_Addition()    # addition of data in the lists for training and testing\n",
        "\n",
        "print( \"Training : \" , len(x_train) , \"Testing : \" , len(x_test) )\n",
        "\n",
        "# checking the lists\n",
        "print( x_train[:2])\n",
        "print( y_train[:2])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLUZ0oKSvewx"
      },
      "source": [
        "# copy the prev cell  ******** Aritra *********\n",
        "# Addition of data into list from CSV for training and public testing\n",
        "def data_Addition() :\n",
        "    global x_train , y_train , x_test , y_test , data_set\n",
        "    d = {3:0} \n",
        "    for row_count,row in data_set.iterrows():\n",
        "        if row['emotion'] in [1,2] or ( row['emotion'] == 3  and d[ row['emotion'] ] > 4000  and row['Usage'] == 'Training' ) : continue \n",
        "        value = row['pixels'].split(' ')   # extracting the pixels as a list\n",
        "        try :\n",
        "            if 'Training' in row['Usage'] :                      # if the current column is for Training\n",
        "\n",
        "                x_train.append(np.array(value,'float32'))        # adding the pixels in x axis\n",
        "                y_train.append(row['emotion'])                   # adding the actual emotion tag in y axis \n",
        "\n",
        "                if row['emotion'] in d :\n",
        "                    d[ row['emotion'] ]+=1\n",
        "                else :\n",
        "                    d[ row['emotion'] ] = 1 \n",
        "            else : \n",
        "\n",
        "                x_test.append(np.array( value,'float32'))\n",
        "                y_test.append( row['emotion'])  \n",
        "\n",
        "        except:\n",
        "            print(\" Error occurred at row number \" + row_count)\n",
        "            print(\"Data Set in that row is \" + row )\n",
        "\n",
        "    print(d)\n",
        "\n",
        "# Now we will do training and public testing\n",
        "\n",
        "x_train , y_train = [] , []   # data the will be used for training will added in this two lists\n",
        "x_test , y_test = [] , []     # data that will be used for public testing will be added here\n",
        "\n",
        "data_Addition()    # addition of data in the lists for training and testing\n",
        "\n",
        "print( \"Training : \" , len(x_train) , \"Testing : \" , len(x_test) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0LndI84mcTI"
      },
      "source": [
        "Don't execute the next as it is the copy of the previous code cell "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ3Vac4qmnXs"
      },
      "source": [
        "# Addition of data into list from CSV for training and public testing\n",
        "'''\n",
        "def data_Addition() :\n",
        "    global x_train , y_train , x_test , y_test , data_set\n",
        "    d = dict()\n",
        "\n",
        "    for row_count,row in data_set.iterrows():\n",
        "        if row['emotion'] in [1,2] :  continue \n",
        "        value = row['pixels'].split(' ')   # extracting the pixels as a list\n",
        "        try :\n",
        "            if 'Training' in row['Usage'] :        # if the current column is for Training\n",
        "                x_train.append(np.array(value,'float32'))        # adding the pixels in the x axis\n",
        "                y_train.append(row['emotion'])\n",
        "                \n",
        "                if row['emotion'] in d :\n",
        "                    d[ row['emotion'] ]+=1\n",
        "                else :\n",
        "                    d[ row['emotion'] ] = 1 \n",
        "            \n",
        "            # adding emotion in the y axis\n",
        "            elif 'PublicTest' in row['Usage']:    # if the current column is for testing\n",
        "                x_test.append(np.array( value,'float32'))\n",
        "                y_test.append( row['emotion'])\n",
        "\n",
        "        except:\n",
        "            print(\" Error occurred at row number \" + row_count)\n",
        "            print(\"Data Set in that row is \" + row )\n",
        "\n",
        "    print(d )\n",
        "\n",
        "# Now we will do training and public testing\n",
        "\n",
        "x_train , y_train = [] , []   # data the will be used for training will added in this two lists\n",
        "x_test , y_test = [] , []     # data that will be used for public testing will be added here\n",
        "\n",
        "data_Addition()    # addition of data in the lists for training and testing\n",
        "\n",
        "# checking the lists\n",
        "#print( x_train[:2])\n",
        "#print( y_train[:2])\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqgWrDBnuynx"
      },
      "source": [
        "As the keras module only takes numpy array as input parameters , we need to convert the lists into numpy arrays ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgIo-D7ivIHX"
      },
      "source": [
        "# Method to Convert from list to Numpy Arrays\n",
        "def Convert_to_np_Array():\n",
        "    global x_train, x_test, y_test , y_train\n",
        "    # Converting list to numpy Array\n",
        "    x_train = np.array(x_train, 'float32')\n",
        "    y_train = np.array(y_train, 'float32')\n",
        "    x_test = np.array(x_test, 'float32')\n",
        "    y_test = np.array(y_test, 'float32')\n",
        "\n",
        "\n",
        "\n",
        "# As the Keras Module only takes numpy arrays as input\n",
        "# we need to convert this lists into numpy arrays\n",
        "\n",
        "Convert_to_np_Array()\n",
        "print( type(x_train) )  # checking if the type has changed to numpy or not \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLkmI8P1GNte"
      },
      "source": [
        "Resaclling of x_train and y_train and then reshaping them into one d array \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRvUpQEcGRhU"
      },
      "source": [
        "def Rescale():\n",
        "    # Normalizing the data\n",
        "    # why data normalization is required - https://www.import.io/post/what-is-data-normalization-and-why-is-it-important/\n",
        "    # how it's work - https://www.educative.io/edpresso/data-normalization-in-python\n",
        "    # read Out - https://www.mathsisfun.com/data/standard-deviation.html\n",
        "\n",
        "    global x_train , x_test , y_test , y_train\n",
        "\n",
        "    # we are basically rescaling\n",
        "    x_train -= np.mean(x_train, axis=0)\n",
        "    x_train /= np.std(x_train, axis=0)  # CENTRALIZING THE DATA\n",
        "\n",
        "    x_test -= np.mean(x_test, axis=0)\n",
        "    x_test /= np.std(x_test, axis=0)\n",
        "\n",
        "\n",
        "def Reshape( width , height ):\n",
        "\n",
        "    global x_train , y_train , x_test , y_test\n",
        "\n",
        "    x_train = x_train.reshape(x_train.shape[0], width, height, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], width, height, 1)\n",
        "    # WHAT THIS FUNCTION DOES TO_CATEGORICAL\n",
        "    y_train = np_utils.to_categorical(y_train,num_classes=7)\n",
        "    y_test = np_utils.to_categorical(y_test,num_classes=7)\n",
        "    \n",
        "    print( ' Reshape method is called  ')\n",
        "\n",
        "# Rescalling the data\n",
        "Rescale()\n",
        "\n",
        "# Reshaping the x train and y train in to a one d array\n",
        "Reshape(48,48)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAysuHTmG5Gs"
      },
      "source": [
        "Now the main comes - designing the CNN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89pfeVebG8cm"
      },
      "source": [
        "\n",
        "def Design_CNN():\n",
        "\n",
        "    # The number of epochs is a hyperparameter\n",
        "    # that defines the number times that the learning algorithm will work through the entire training dataset\n",
        "    features = 64\n",
        "    Batch_size = 64\n",
        "    Label = 7\n",
        "    epoch = 100\n",
        "    global  x_train, y_train\n",
        "    model = Sequential()\n",
        "\n",
        "    ## Layer 1\n",
        "    # adding layers\n",
        "    # Conv2d is used as the image are in 2d format\n",
        "    # here we are trying extract input\n",
        "    # Relu is a rectifier\n",
        "\n",
        "    # Search Kernal size\n",
        "    model.add(Conv2D(features,kernel_size=(3,3),activation='relu',input_shape=(x_train.shape[1:])))\n",
        "    model.add(Conv2D(features,kernel_size=(3, 3),activation='relu'))\n",
        "\n",
        "    # adding a max pooling 2D layer\n",
        "    # It mainly helps to control over fitting\n",
        "    # can use average pooling layer also\n",
        "    model.add( MaxPool2D(pool_size=(2,2),strides=(2,2)) )\n",
        "\n",
        "    # adding a drop out layer\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    ## 2ND layer\n",
        "    model.add(Conv2D(features, kernel_size=(3, 3), activation='relu'))\n",
        "    model.add(Conv2D(features, kernel_size=(3, 3), activation='relu'))\n",
        "    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    ## 3RD Layer\n",
        "    model.add(Conv2D(2*features, kernel_size=(3, 3), activation='relu'))\n",
        "    model.add(Conv2D(2*features, kernel_size=(3, 3), activation='relu'))\n",
        "    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add( Flatten() )\n",
        "\n",
        "    # adding dense layers\n",
        "    model.add(Dense(2**3 * features, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(2 ** 3 * features, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Adding the final layers\n",
        "    model.add(Dense(Label,activation='softmax')) # Activation is softmax as we want to bind in the 7 labels of 0ptions\n",
        "\n",
        "    model.compile(loss=categorical_crossentropy,optimizer=Adam(),metrics=['accuracy'])\n",
        "    model.fit(x_train,y_train,batch_size=Batch_size,epochs=epoch,verbose=1,validation_data=(x_test,y_test), shuffle=True )\n",
        "\n",
        "    # Saving the model\n",
        "\n",
        "    EmotionDetectJson = model.to_json()\n",
        "    with open(\"fer.json\",\"w\") as file :\n",
        "        file.write(EmotionDetectJson)\n",
        "    model.save_weights(\"fer.h5\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Start designing Our CNN\n",
        "\n",
        "# To build the model we will be using sequential Type\n",
        "\n",
        "Design_CNN()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}